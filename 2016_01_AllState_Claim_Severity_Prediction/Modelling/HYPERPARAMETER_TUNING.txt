==============================================
GRADIANT BOOSTING MACHINE - PARAMETERS TO TUNE
==============================================

gbm <- h2o.gbm(
  ## standard model parameters
  x = predictors, 
  y = response, 
  training_frame = train, 
  validation_frame = valid,

  ## more trees is better if the learning rate is small enough 
  ## here, use "more than enough" trees - we have early stopping
  ntrees = 10000,                                                            

  ## smaller learning rate is better (this is a good value for most datasets, but see below for annealing)
  learn_rate=0.01,                                                         

  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "AUC", 

  ## sample 80% of rows per tree
  sample_rate = 0.8,                                                       

  ## sample 80% of columns per split
  col_sample_rate = 0.8,                                                   

  ## fix a random number generator seed for reproducibility
  seed = 1234,                                                             

  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10                                                 
)

------------------
PARAMETERS TO TUNE
------------------
LEVEL_01:

ntrees = 100/LearningRate
learn_rate = 0.01-0.1 IN STEPS OF 0.01
max_depth = 4/5 - MAX=10

LEVEL_02:

sample_rate = 0.8
col_sample_rate = 0.7

------------------------------------
RANDOM DISCRETE - SEARCH OBSEVRATION
------------------------------------

1. At LearningRate = 0.15 | MaxDepth = 10 >> The learning stbailizes arodun 270 trees
2. At LearningRate = 0.03 | MaxDepth = 10 >> The learning stbailizes arodun 880 trees
3. stopping_metric = "MSE" does not seem to be working >> Use "deviance" instead
4. Learning rate > 0.15 does not stabilizes the error and have a tendancy overfit the training data >> Test error increases rapidly for > 0.15 Learning rate
5. Learning rate = 0.02 | MaxDepth = 10 >> Stabilize ~600 Trees - Increase thereafter
6. Learning rate = 0.05 | MaxDepth = 10 >> Stabilize ~235 Trees - Increase thereafter
7. Learning rate = 0.06 | MaxDepth = 10 >> Stabilize ~185 Trees - Increase thereafter
8. Learning rate = 0.03 | MaxDepth = 10 >> Stabilize ~385-~465 Trees - Increase thereafter | Broad rnage stability
9. OVERALL: Lower learning rate models are running better than higher. Keep LR Range: 0.0-0.1
10: LR 0.00-0.05 [For 2 decimal significance] >> NTRESS: 200-600
	LR 0.05-0.10 [For 2 decimal significance] >> NTRESS: 100-250
	
--------------------
INFORMATION EXTRACT
--------------------
ntrees
max_depth
learn_rate
learn_rate_annealing
col_sample_rate
stopping_rounds
stopping_metric
stopping_tolerance
score_tree_interval
distribution
Min_Val_Dev
TrainDev@Min_Val_Dev
Min_Val_MAE
TrainMAE@Min_Val_MAE
ntree@Min_Val_Dev
ntree@Min_Val_MAE

------------------
4 GRIDs: 36-Models
------------------
GRID_LEARNRATE_NOANNEAL_2
GRID_LEARNRATE_NOANNEAL_3
GRID_LEARNRATE_NOANNEAL_4
GRID_LEARNRATE_NOANNEAL_5


