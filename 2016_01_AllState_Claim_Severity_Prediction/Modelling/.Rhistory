x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_GBM_04",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
# ntrees and max_depth selected based on the results from past tuning
ntrees = 1500,
#max_depth = 8,
## LR selected based on past tuning experience
#learn_rate = 0.02,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## learn_rate_annealing = 0.99,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 1e-4,
stopping_metric = "deviance",
## sample 80% of rows per tree
sample_rate = 1,
## sample 80% of columns per split
##col_sample_rate = 0.8,
## score every 5 trees to make early stopping reproducible
score_tree_interval = 5)
# ----------------------------------------------------
# LIST OF GRIDS FROM WHICH DATA NEEDS TO BE EXTRACTED
# ----------------------------------------------------
#GRID_Ids = c("GRID_LEARNRATE_NOANNEAL_2", "GRID_LEARNRATE_NOANNEAL_3", "GRID_LEARNRATE_NOANNEAL_4", "GRID_LEARNRATE_NOANNEAL_5", "GRID_LEARNRATE_NOANNEAL_6")
#GRID_Ids = c("KAGGLE_GRID_4")
#GRID_Ids = c("GBM_GRID_LR_ANNEAL_1", "KAGGLE_GRID_4")
#GRID_Ids = c("GBM_GRID_LR_ANNEAL_1")
GRID_Ids = c("GRID_GBM_04")
#-----------------------------------------
# EMPTY DATA FRAMES FOR CONSOLIDATING DATA
#-----------------------------------------
dfFinal = data.frame()
dfImpVars = data.frame()
# EXTRACT DATA FOR ALL GRIDS IN "GRID_IDs"
for(i in 1:length(GRID_Ids))
{
# OBTAIN GRID AND "THE BEST" MODEL OF THAT GRID
GRID = h2o.getGrid(grid_id = GRID_Ids[i], sort_by = "mse")
TopMod = h2o.getModel(model_id = GRID@model_ids[[1]])
# EXTRACT MODEL PARAMETERS FOR EACH MODEL IN THE GRID
ModelName = unlist(GRID@model_ids)
ntrees = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$ntrees}))
max_depth = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$max_depth}))
learn_rate = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$learn_rate}))
learn_rate_annealing = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$learn_rate_annealing}))
col_sample_rate = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$col_sample_rate}))
stopping_rounds = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_rounds}))
stopping_metric = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_metric}))
stopping_tolerance = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_tolerance}))
score_tree_interval  = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$score_tree_interval}))
distribution = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$distribution}))
# EXTRACT IMPORTANT PARAMETERS FOR SCORING ROUND OF EACH MODEL IN THE GRID
Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){min(y@model$scoring_history[,"validation_deviance"])}))
TrainDev_At_Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$training_deviance[which.min(y@model$scoring_history[,"validation_deviance"])]}))
ntree_At_Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$number_of_trees[which.min(y@model$scoring_history[,"validation_deviance"])]}))
Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){min(y@model$scoring_history[,"validation_mae"])}))
TrainMAE_At_Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$training_mae[which.min(y@model$scoring_history[,"validation_mae"])]}))
ntree_At_Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$number_of_trees[which.min(y@model$scoring_history[,"validation_mae"])]}))
MaxTreesBuilt = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){max(y@model$scoring_history$number_of_trees)}))
# COMBINE EXTRACTED RESULTS VERTICALLY
df1 = cbind(ModelName, ntrees, max_depth, learn_rate, learn_rate_annealing, col_sample_rate, stopping_rounds, stopping_metric, stopping_tolerance,
score_tree_interval, distribution, Min_Val_Dev, TrainDev_At_Min_Val_Dev, ntree_At_Min_Val_Dev, Min_Val_MAE, TrainMAE_At_Min_Val_MAE,
ntree_At_Min_Val_MAE, MaxTreesBuilt)
# UPDATE FINAL DATA FRAMES: MODEL PARAMETERS + SCORING DATA + IMPORTANT VARIABLS
dfFinal = rbind(dfFinal, as.data.frame(df1))
dfImpVars = rbind(dfImpVars, TopMod@model$variable_importances)
}
# WRITE TO DISK
write.csv(x = dfFinal, file = "GBM_TUNING_PARAMETERS.csv", row.names = FALSE)
write.csv(x = dfImpVars, file = "GBM_TOP_MODEL_VARIABLE_IMPORTANCE.csv", row.names = FALSE)
GBMModel = h2o.getModel(model_id = "GRID_GBM_02_model_8")
GBMModel
GBMModel = h2o.getModel(model_id = "GRID_GBM_02_model_8")
predGBM = h2o.predict(object = GBMModel, newdata = AllStateTest.hex)
# Following step is required only for Log(loss) predictions
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_05112016_01.csv", row.names = FALSE)
# Generating Predicting For Top 3 Models:
# #3: GRID_GBM_02_model_8
# #2: GRID_GBM_04_model_1
# #1: GRID_GBM_04_model_0
# FOR #3: GRID_GBM_02_model_8
GBMModel = h2o.getModel(model_id = "GRID_GBM_02_model_8")
predGBM = h2o.predict(object = GBMModel, newdata = AllStateTest.hex)
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_05112016_01.csv", row.names = FALSE)
# FOR #3: GRID_GBM_04_model_1
GBMModel = h2o.getModel(model_id = "GRID_GBM_04_model_1")
predGBM = h2o.predict(object = GBMModel, newdata = AllStateTest.hex)
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_05112016_02.csv", row.names = FALSE)
# FOR #3: GRID_GBM_04_model_0
GBMModel = h2o.getModel(model_id = "GRID_GBM_04_model_0")
predGBM = h2o.predict(object = GBMModel, newdata = AllStateTest.hex)
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_05112016_03.csv", row.names = FALSE)
# Cleanup the environment and load libraries
rm(list = ls(all.names = TRUE))
library(h2o)
library(dplyr)
library(xgboost)
# Initialize H2O :: DELL_LAPTOP
h2o.init(nthreads = -1, min_mem_size = "5G")
# Initialize H2O :: LENOVO_AIO
#h2o.init(nthreads = -1, min_mem_size = "3500M")
# Read Train & Test into H2O - DELL_LAPTOP
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Read Train & Test into H2O - LENOVO_AIO
#AllStateTrain.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
#AllStateTest.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Transform "loss" variable to Log(loss)
AllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)
# No loss variable in AllStateTest.hex
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
### REMOVING UNWANTED FEATURES - BASED ON GBM FEATURE SELECTION
### FEATURE ELIMINATION: AGGRESIVE [REMOVING 61 FEATURES]
-------------------------------------------------------
#ncol(AllStateTrain.hex)
#AllStateTrain.hex = as.h2o(select(as.data.frame(AllStateTrain.hex), -cat89, -cat104, -cat50, -cat97, -cat52, -cat10, -cat92, -cat7, -cat3, -cat65, -cat66, -cat8, -cat67, -cat42, -cat54, -cat95, -cat32, -cat19, -cat78, -cat21, -cat29, -cat51, -cat85, -cat40, -cat16, -cat41, -cat28, -cat45, -cat43, -cat98, -cat30, -cat74, -cat24, -cat88, -cat86, -cat96, -cat18, -cat59, -cat17, -cat14, -cat56, -cat71, -cat33, -cat34, -cat61, -cat46, -cat68, -cat60, -cat47, -cat63, -cat48, -cat35, -cat22, -cat55, -cat58, -cat20, -cat62, -cat70, -cat15, -cat69, -cat64))
#ncol(AllStateTrain.hex)
#ncol(AllStateTest.hex)
#AllStateTest.hex = as.h2o(select(as.data.frame(AllStateTest.hex), -cat89, -cat104, -cat50, -cat97, -cat52, -cat10, -cat92, -cat7, -cat3, -cat65, -cat66, -cat8, -cat67, -cat42, -cat54, -cat95, -cat32, -cat19, -cat78, -cat21, -cat29, -cat51, -cat85, -cat40, -cat16, -cat41, -cat28, -cat45, -cat43, -cat98, -cat30, -cat74, -cat24, -cat88, -cat86, -cat96, -cat18, -cat59, -cat17, -cat14, -cat56, -cat71, -cat33, -cat34, -cat61, -cat46, -cat68, -cat60, -cat47, -cat63, -cat48, -cat35, -cat22, -cat55, -cat58, -cat20, -cat62, -cat70, -cat15, -cat69, -cat64))
#ncol(AllStateTest.hex)
### FEATURE ELIMINATION: MODERATE [REMOVED 31 FEATURES]
-------------------------------------------------------
#ncol(AllStateTrain.hex)
#AllStateTrain.hex = as.h2o(select(as.data.frame(AllStateTrain.hex), -cat30,  -cat74,  -cat24,  -cat88,  -cat86,  -cat96,  -cat18,  -cat59,  -cat17,  -cat14,  -cat56,  -cat71,  -cat33,  -cat34,  -cat61,  -cat46,  -cat68,  -cat60,  -cat47,  -cat63,  -cat48,  -cat35,  -cat22,  -cat55,  -cat58,  -cat20,  -cat62,  -cat70,  -cat15,  -cat69,  -cat64))
#ncol(AllStateTrain.hex)
#ncol(AllStateTest.hex)
#AllStateTest.hex = as.h2o(select(as.data.frame(AllStateTest.hex), -cat30,  -cat74,  -cat24,  -cat88,  -cat86,  -cat96,  -cat18,  -cat59,  -cat17,  -cat14,  -cat56,  -cat71,  -cat33,  -cat34,  -cat61,  -cat46,  -cat68,  -cat60,  -cat47,  -cat63,  -cat48,  -cat35,  -cat22,  -cat55,  -cat58,  -cat20,  -cat62,  -cat70,  -cat15,  -cat69,  -cat64))
#ncol(AllStateTest.hex)
dfAllStateTrain = as.data.frame(x = AllStateTrain.hex)
dfAllStateTest = as.data.frame(x = AllStateTest.hex)
h2o.shutdown(prompt = FALSE)
# Cleanup the environment and load libraries
rm(list = ls(all.names = TRUE))
library(h2o)
library(dplyr)
library(xgboost)
# Initialize H2O :: DELL_LAPTOP
h2o.init(nthreads = -1, min_mem_size = "5G")
# Initialize H2O :: LENOVO_AIO
#h2o.init(nthreads = -1, min_mem_size = "3500M")
# Read Train & Test into H2O - DELL_LAPTOP
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Read Train & Test into H2O - LENOVO_AIO
#AllStateTrain.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
#AllStateTest.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Transform "loss" variable to Log(loss)
AllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)
# No loss variable in AllStateTest.hex
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
### REMOVING UNWANTED FEATURES - BASED ON GBM FEATURE SELECTION
### FEATURE ELIMINATION: AGGRESIVE [REMOVING 61 FEATURES]
-------------------------------------------------------
#ncol(AllStateTrain.hex)
#AllStateTrain.hex = as.h2o(select(as.data.frame(AllStateTrain.hex), -cat89, -cat104, -cat50, -cat97, -cat52, -cat10, -cat92, -cat7, -cat3, -cat65, -cat66, -cat8, -cat67, -cat42, -cat54, -cat95, -cat32, -cat19, -cat78, -cat21, -cat29, -cat51, -cat85, -cat40, -cat16, -cat41, -cat28, -cat45, -cat43, -cat98, -cat30, -cat74, -cat24, -cat88, -cat86, -cat96, -cat18, -cat59, -cat17, -cat14, -cat56, -cat71, -cat33, -cat34, -cat61, -cat46, -cat68, -cat60, -cat47, -cat63, -cat48, -cat35, -cat22, -cat55, -cat58, -cat20, -cat62, -cat70, -cat15, -cat69, -cat64))
#ncol(AllStateTrain.hex)
#ncol(AllStateTest.hex)
#AllStateTest.hex = as.h2o(select(as.data.frame(AllStateTest.hex), -cat89, -cat104, -cat50, -cat97, -cat52, -cat10, -cat92, -cat7, -cat3, -cat65, -cat66, -cat8, -cat67, -cat42, -cat54, -cat95, -cat32, -cat19, -cat78, -cat21, -cat29, -cat51, -cat85, -cat40, -cat16, -cat41, -cat28, -cat45, -cat43, -cat98, -cat30, -cat74, -cat24, -cat88, -cat86, -cat96, -cat18, -cat59, -cat17, -cat14, -cat56, -cat71, -cat33, -cat34, -cat61, -cat46, -cat68, -cat60, -cat47, -cat63, -cat48, -cat35, -cat22, -cat55, -cat58, -cat20, -cat62, -cat70, -cat15, -cat69, -cat64))
#ncol(AllStateTest.hex)
dfAllStateTrain = as.data.frame(x = AllStateTrain.hex)
dfAllStateTest = as.data.frame(x = AllStateTest.hex)
h2o.shutdown(prompt = FALSE)
names(dfAllStateTrain)
summary(dfAllStateTrain$loss)
# Cleanup the environment and load libraries
rm(list = ls(all.names = TRUE))
library(h2o)
library(dplyr)
library(xgboost)
# Initialize H2O :: DELL_LAPTOP
h2o.init(nthreads = -1, min_mem_size = "5G")
# Initialize H2O :: LENOVO_AIO
#h2o.init(nthreads = -1, min_mem_size = "3500M")
# Read Train & Test into H2O - DELL_LAPTOP
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Read Train & Test into H2O - LENOVO_AIO
#AllStateTrain.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
#AllStateTest.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
dfAllStateTrain = as.data.frame(x = AllStateTrain.hex)
dfAllStateTest = as.data.frame(x = AllStateTest.hex)
exp(1)
log(1)
log(2.718282)
# No loss variable in AllStateTest.hex
# Shutdown H2O after PCA is done !
h2o.shutdown(prompt = FALSE)
# Transform "loss" variable to Log(loss)
dfAllStateTrain$loss = log(dfAllStateTrain$loss)
# No loss variable in AllStateTest.hex
# Split Training Data into Train & Validation set
set.seed(1)
SRS = sample(x = 1:nrow(dfAllStateTrain), size = 0.7*nrow(dfAllStateTrain))
dfModTrain = dfAllStateTrain[SRS, ]
dfModTest = dfAllStateTrain[-SRS, ]
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(ModTrain.hex), DepAttrib)
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(dfModTrain), DepAttrib)
"loss" %in% names(dfAllStateTest)
library(dummies)
names(dfAllStateTrain)
dfTemp = dummy.data.frame(data = dfAllStateTrain, names = names(dfAllStateTrain)[1:116])
names(dfTemp)
str(dfTemp)
summary(dfTemp)
plot(dfTemp$loss)
names(dfAllStateTest)
# Convert Train and Test data frames into numerical
dfAllStateTrain = dummy.data.frame(data = dfAllStateTrain, names = names(dfAllStateTrain)[1:116])
dfAllStateTest = dummy.data.frame(data = dfAllStateTest, names = names(dfAllStateTest)[1:116])
# Convert Train and Test data frames into numerical
dfAllStateTrain = dummy.data.frame(data = dfAllStateTrain, names = names(dfAllStateTrain)[1:116])
library(h2o)
library(dplyr)
library(xgboost)
library(dummies)
# Convert Train and Test data frames into numerical
dfAllStateTrain = dummy.data.frame(data = dfAllStateTrain, names = names(dfAllStateTrain)[1:116])
dfAllStateTest = dummy.data.frame(data = dfAllStateTest, names = names(dfAllStateTest)[1:116])
dfAllStateTest = dummy.data.frame(data = dfAllStateTest, names = names(dfAllStateTest)[1:116])
# Split Training Data into Train & Validation set
set.seed(1)
SRS = sample(x = 1:nrow(dfAllStateTrain), size = 0.7*nrow(dfAllStateTrain))
dfModTrain = dfAllStateTrain[SRS, ]
dfModTest = dfAllStateTrain[-SRS, ]
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(dfModTrain), DepAttrib)
exp(0.42)
xgbParams = list(objective = "reg:linear", eta = 0.3, max.depth = 5, subsample = 0.5, colsample_bytree = 0.5)
xgbModel001 = xgboost(data = data.matrix(frame = dfModTrain), label = "loss", params = xbgParams, nrounds = 5, verbose = 2, early.stop.round = 3,maximize = TRUE )
xgbParams = list(objective = "reg:linear", eta = 0.3, max.depth = 5, subsample = 0.5, colsample_bytree = 0.5)
xgbModel001 = xgboost(data = data.matrix(frame = dfModTrain), label = "loss", params = xbgParams, nrounds = 1, verbose = 2, early.stop.round = 3,maximize = TRUE )
memory.size()
gc()
memory.size()
gc(verbose = TRUE)
xgbParams = list(objective = "reg:linear", eta = 0.3, max.depth = 5, subsample = 0.5, colsample_bytree = 0.5)
xgbModel001 = xgboost(data = data.matrix(frame = dfModTrain), label = "loss", params = xbgParams, nrounds = 1, verbose = 2, early.stop.round = 3,maximize = TRUE )
# Cleanup the environment and load libraries
rm(list = ls(all.names = TRUE))
library(h2o)
library(dplyr)
# Initialize H2O :: DELL_LAPTOP
h2o.init(nthreads = -1, min_mem_size = "7G")
# Initialize H2O :: LENOVO_AIO
#h2o.init(nthreads = -1, min_mem_size = "3500M")
# Read Train & Test into H2O - DELL_LAPTOP
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Read Train & Test into H2O - LENOVO_AIO
#AllStateTrain.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
#AllStateTest.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Transform "loss" variable to Log(loss)
AllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)
# No loss variable in AllStateTest.hex
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
### REMOVING UNWANTED FEATURES - BASED ON GBM FEATURE SELECTION
### FEATURE ELIMINATION: AGGRESIVE [REMOVING 61 FEATURES]
#ncol(AllStateTrain.hex)
#AllStateTrain.hex = as.h2o(select(as.data.frame(AllStateTrain.hex), -cat89, -cat104, -cat50, -cat97, -cat52, -cat10, -cat92, -cat7, -cat3, -cat65, -cat66, -cat8, -cat67, -cat42, -cat54, -cat95, -cat32, -cat19, -cat78, -cat21, -cat29, -cat51, -cat85, -cat40, -cat16, -cat41, -cat28, -cat45, -cat43, -cat98, -cat30, -cat74, -cat24, -cat88, -cat86, -cat96, -cat18, -cat59, -cat17, -cat14, -cat56, -cat71, -cat33, -cat34, -cat61, -cat46, -cat68, -cat60, -cat47, -cat63, -cat48, -cat35, -cat22, -cat55, -cat58, -cat20, -cat62, -cat70, -cat15, -cat69, -cat64))
#ncol(AllStateTrain.hex)
#ncol(AllStateTest.hex)
#AllStateTest.hex = as.h2o(select(as.data.frame(AllStateTest.hex), -cat89, -cat104, -cat50, -cat97, -cat52, -cat10, -cat92, -cat7, -cat3, -cat65, -cat66, -cat8, -cat67, -cat42, -cat54, -cat95, -cat32, -cat19, -cat78, -cat21, -cat29, -cat51, -cat85, -cat40, -cat16, -cat41, -cat28, -cat45, -cat43, -cat98, -cat30, -cat74, -cat24, -cat88, -cat86, -cat96, -cat18, -cat59, -cat17, -cat14, -cat56, -cat71, -cat33, -cat34, -cat61, -cat46, -cat68, -cat60, -cat47, -cat63, -cat48, -cat35, -cat22, -cat55, -cat58, -cat20, -cat62, -cat70, -cat15, -cat69, -cat64))
#ncol(AllStateTest.hex)
### FEATURE ELIMINATION: MODERATE [REMOVED 31 FEATURES]
ncol(AllStateTrain.hex)
AllStateTrain.hex = as.h2o(select(as.data.frame(AllStateTrain.hex), -cat30,  -cat74,  -cat24,  -cat88,  -cat86,  -cat96,  -cat18,  -cat59,  -cat17,  -cat14,  -cat56,  -cat71,  -cat33,  -cat34,  -cat61,  -cat46,  -cat68,  -cat60,  -cat47,  -cat63,  -cat48,  -cat35,  -cat22,  -cat55,  -cat58,  -cat20,  -cat62,  -cat70,  -cat15,  -cat69,  -cat64))
ncol(AllStateTrain.hex)
ncol(AllStateTest.hex)
AllStateTest.hex = as.h2o(select(as.data.frame(AllStateTest.hex), -cat30,  -cat74,  -cat24,  -cat88,  -cat86,  -cat96,  -cat18,  -cat59,  -cat17,  -cat14,  -cat56,  -cat71,  -cat33,  -cat34,  -cat61,  -cat46,  -cat68,  -cat60,  -cat47,  -cat63,  -cat48,  -cat35,  -cat22,  -cat55,  -cat58,  -cat20,  -cat62,  -cat70,  -cat15,  -cat69,  -cat64))
ncol(AllStateTest.hex)
# Split Training Data into Train & Validation set
SplitFrames = h2o.splitFrame(data = AllStateTrain.hex, ratios = 0.7, seed = 1)
ModTrain.hex = h2o.assign(data = SplitFrames[[1]], key = "ModTrain.hex")
ModTest.hex = h2o.assign(data = SplitFrames[[2]], key = "ModTest.hex")
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(ModTrain.hex), DepAttrib)
# Hyperparameters for Round 2:
HyperParam = list(col_sample_rate = c(0.25, 0.35, 0.5), sample_rate = c(0.4,0.5,0.6,0.7,0.8,0.9,1))
gridGBM          <- h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_GBM_05",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
# ntrees and max_depth selected based on the results from past tuning
ntrees = 1500,
max_depth = 6,
learn_rate = 0.02,
## learn_rate_annealing = 0.99,
stopping_rounds = 3,
stopping_tolerance = 1e-4,
stopping_metric = "deviance",
#sample_rate = 1,
## col_sample_rate = 0.8,
## score every 5 trees to make early stopping reproducible
score_tree_interval = 5)
h2o.clusterIsUp()
# ----------------------------------------------------
# LIST OF GRIDS FROM WHICH DATA NEEDS TO BE EXTRACTED
# ----------------------------------------------------
#GRID_Ids = c("GRID_LEARNRATE_NOANNEAL_2", "GRID_LEARNRATE_NOANNEAL_3", "GRID_LEARNRATE_NOANNEAL_4", "GRID_LEARNRATE_NOANNEAL_5", "GRID_LEARNRATE_NOANNEAL_6")
#GRID_Ids = c("KAGGLE_GRID_4")
#GRID_Ids = c("GBM_GRID_LR_ANNEAL_1", "KAGGLE_GRID_4")
#GRID_Ids = c("GBM_GRID_LR_ANNEAL_1")
GRID_Ids = c("GRID_GBM_05")
#-----------------------------------------
# EMPTY DATA FRAMES FOR CONSOLIDATING DATA
#-----------------------------------------
dfFinal = data.frame()
dfImpVars = data.frame()
# EXTRACT DATA FOR ALL GRIDS IN "GRID_IDs"
for(i in 1:length(GRID_Ids))
{
# OBTAIN GRID AND "THE BEST" MODEL OF THAT GRID
GRID = h2o.getGrid(grid_id = GRID_Ids[i], sort_by = "mse")
TopMod = h2o.getModel(model_id = GRID@model_ids[[1]])
# EXTRACT MODEL PARAMETERS FOR EACH MODEL IN THE GRID
ModelName = unlist(GRID@model_ids)
ntrees = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$ntrees}))
max_depth = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$max_depth}))
learn_rate = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$learn_rate}))
learn_rate_annealing = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$learn_rate_annealing}))
col_sample_rate = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$col_sample_rate}))
stopping_rounds = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_rounds}))
stopping_metric = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_metric}))
stopping_tolerance = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_tolerance}))
score_tree_interval  = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$score_tree_interval}))
distribution = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$distribution}))
# EXTRACT IMPORTANT PARAMETERS FOR SCORING ROUND OF EACH MODEL IN THE GRID
Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){min(y@model$scoring_history[,"validation_deviance"])}))
TrainDev_At_Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$training_deviance[which.min(y@model$scoring_history[,"validation_deviance"])]}))
ntree_At_Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$number_of_trees[which.min(y@model$scoring_history[,"validation_deviance"])]}))
Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){min(y@model$scoring_history[,"validation_mae"])}))
TrainMAE_At_Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$training_mae[which.min(y@model$scoring_history[,"validation_mae"])]}))
ntree_At_Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$number_of_trees[which.min(y@model$scoring_history[,"validation_mae"])]}))
MaxTreesBuilt = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){max(y@model$scoring_history$number_of_trees)}))
# COMBINE EXTRACTED RESULTS VERTICALLY
df1 = cbind(ModelName, ntrees, max_depth, learn_rate, learn_rate_annealing, col_sample_rate, stopping_rounds, stopping_metric, stopping_tolerance,
score_tree_interval, distribution, Min_Val_Dev, TrainDev_At_Min_Val_Dev, ntree_At_Min_Val_Dev, Min_Val_MAE, TrainMAE_At_Min_Val_MAE,
ntree_At_Min_Val_MAE, MaxTreesBuilt)
# UPDATE FINAL DATA FRAMES: MODEL PARAMETERS + SCORING DATA + IMPORTANT VARIABLS
dfFinal = rbind(dfFinal, as.data.frame(df1))
dfImpVars = rbind(dfImpVars, TopMod@model$variable_importances)
}
# WRITE TO DISK
write.csv(x = dfFinal, file = "GBM_TUNING_PARAMETERS.csv", row.names = FALSE)
write.csv(x = dfImpVars, file = "GBM_TOP_MODEL_VARIABLE_IMPORTANCE.csv", row.names = FALSE)
# ----------------------------------------------------
# LIST OF GRIDS FROM WHICH DATA NEEDS TO BE EXTRACTED
# ----------------------------------------------------
#GRID_Ids = c("GRID_LEARNRATE_NOANNEAL_2", "GRID_LEARNRATE_NOANNEAL_3", "GRID_LEARNRATE_NOANNEAL_4", "GRID_LEARNRATE_NOANNEAL_5", "GRID_LEARNRATE_NOANNEAL_6")
#GRID_Ids = c("KAGGLE_GRID_4")
#GRID_Ids = c("GBM_GRID_LR_ANNEAL_1", "KAGGLE_GRID_4")
#GRID_Ids = c("GBM_GRID_LR_ANNEAL_1")
GRID_Ids = c("GRID_GBM_05")
#-----------------------------------------
# EMPTY DATA FRAMES FOR CONSOLIDATING DATA
#-----------------------------------------
dfFinal = data.frame()
dfImpVars = data.frame()
# EXTRACT DATA FOR ALL GRIDS IN "GRID_IDs"
for(i in 1:length(GRID_Ids))
{
# OBTAIN GRID AND "THE BEST" MODEL OF THAT GRID
GRID = h2o.getGrid(grid_id = GRID_Ids[i], sort_by = "mse")
TopMod = h2o.getModel(model_id = GRID@model_ids[[1]])
# EXTRACT MODEL PARAMETERS FOR EACH MODEL IN THE GRID
ModelName = unlist(GRID@model_ids)
ntrees = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$ntrees}))
max_depth = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$max_depth}))
learn_rate = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$learn_rate}))
learn_rate_annealing = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$learn_rate_annealing}))
col_sample_rate = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$col_sample_rate}))
sample_rate = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$sample_rate}))
stopping_rounds = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_rounds}))
stopping_metric = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_metric}))
stopping_tolerance = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$stopping_tolerance}))
score_tree_interval  = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$score_tree_interval}))
distribution = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}), function(y){y@allparameters$distribution}))
# EXTRACT IMPORTANT PARAMETERS FOR SCORING ROUND OF EACH MODEL IN THE GRID
Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){min(y@model$scoring_history[,"validation_deviance"])}))
TrainDev_At_Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$training_deviance[which.min(y@model$scoring_history[,"validation_deviance"])]}))
ntree_At_Min_Val_Dev = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$number_of_trees[which.min(y@model$scoring_history[,"validation_deviance"])]}))
Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){min(y@model$scoring_history[,"validation_mae"])}))
TrainMAE_At_Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$training_mae[which.min(y@model$scoring_history[,"validation_mae"])]}))
ntree_At_Min_Val_MAE = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){y@model$scoring_history$number_of_trees[which.min(y@model$scoring_history[,"validation_mae"])]}))
MaxTreesBuilt = unlist(lapply(lapply(GRID@model_ids, function(x){h2o.getModel(x)}),
function(y){max(y@model$scoring_history$number_of_trees)}))
# COMBINE EXTRACTED RESULTS VERTICALLY
df1 = cbind(ModelName, ntrees, max_depth, learn_rate, learn_rate_annealing, col_sample_rate, sample_rate, stopping_rounds, stopping_metric, stopping_tolerance, score_tree_interval, distribution, Min_Val_Dev, TrainDev_At_Min_Val_Dev, ntree_At_Min_Val_Dev, Min_Val_MAE, TrainMAE_At_Min_Val_MAE, ntree_At_Min_Val_MAE, MaxTreesBuilt)
# UPDATE FINAL DATA FRAMES: MODEL PARAMETERS + SCORING DATA + IMPORTANT VARIABLS
dfFinal = rbind(dfFinal, as.data.frame(df1))
dfImpVars = rbind(dfImpVars, TopMod@model$variable_importances)
}
# WRITE TO DISK
write.csv(x = dfFinal, file = "GBM_TUNING_PARAMETERS.csv", row.names = FALSE)
write.csv(x = dfImpVars, file = "GBM_TOP_MODEL_VARIABLE_IMPORTANCE.csv", row.names = FALSE)
