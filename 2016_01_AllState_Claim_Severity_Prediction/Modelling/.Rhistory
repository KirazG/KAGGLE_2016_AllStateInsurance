learn_rate_annealing = 0.99,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 1e-4,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 10)
gridDepthSearch
gridDepthSearch@model_ids
h2o.mae(object = h2o.getModel("GRID_DEPTH_model_2"), train = TRUE, valid = TRUE)
gridDepth = h2o.getGrid(grid_id = "GRID_DEPTH", sort_by = "mse")
rgidDepth
gridDepth
gridDepth@summary_table
lapply(gridDepth@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
lapply(gridDepth@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
L <- lapply(gridDepth@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
L[[1]]
L[[1]]["train"]
lappy(lapply(gridDepth@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y["valid"] - y["train"]})
lapply(lapply(gridDepth@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y["valid"] - y["train"]})
gridDepth
lapply(lapply(gridDepth@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y["valid"] - y["train"]})
gridDepth
Id = gridGBM@model_ids[[1]]
TopModel = h2o.getModel(model_id = gridDepth@model_ids[[1]])
h2o.varimp_plot(model = TopModel, num_of_features = 35)
h2o.varimp_plot(model = TopModel, num_of_features = 50)
h2o.varimp(TopModel)
ImpVar <- h2o.varimp(TopModel)
View(ImpVar)
m(ImpVar$percentage)
sum(ImpVar$percentage)
ImpVar = h2o.varimp(TopModel)
write.csv(x = as.data.frame(x = ImpVar), file = "TOP_FEATURES_GBM_GRID_MAX_DEPTH.csv", row.names = FALSE)
for(i in 1:10)
{
ImpVar = h2o.varimp(h2o.getModel(model_id = gridDepth@model_ids[[i]]))
write.csv(x = as.data.frame(x = ImpVar), file = paste0("FEATURES_GBM_GRID_MaxDepth_", i, ".csv"), row.names = FALSE)
}
# Generate Predictions
predGBM = h2o.predict(object = TopModel, newdata = AllStateTest.hex)
predGBM = h2o.predict(object = TopModel, newdata = AllStateTest.hex)
# Following step is required only for Log(loss) predictions
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_26102016_01.csv", row.names = FALSE)
View(dfGBMPredictions)
gridDepth
TopModel = h2o.getModel(model_id = "GRID_DEPTH_model_4")
# Generate Predictions
predGBM = h2o.predict(object = TopModel, newdata = AllStateTest.hex)
# Following step is required only for Log(loss) predictions
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_26102016_01.csv", row.names = FALSE)
write.csv(x = dfGBMPredictions, file = "H2O_GBM_26102016_01.csv", row.names = FALSE)
rm(list = ls(all.names = TRUE))
library(h2o)
# Initialize H2O
h2o.init(nthreads = -1, min_mem_size = "6G")
# Read Train & Test into H2O
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Transform "loss" variable to Log(loss)
AllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)
# No loss variable in AllStateTest.hex
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
# Split Training Data into Train & Validation set
SplitFrames = h2o.splitFrame(data = AllStateTrain.hex, ratios = 0.7, seed = 1)
ModTrain.hex = h2o.assign(data = SplitFrames[[1]], key = "ModTrain.hex")
ModTest.hex = h2o.assign(data = SplitFrames[[2]], key = "ModTest.hex")
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(ModTrain.hex), DepAttrib)
HyperParam = list(ntrees = c(100,200,400,800,1600,3200,6400,12800))
gridTreesSearch <- h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_TOT_TREES",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 5,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
learn_rate_annealing = 0.95,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 1e-4,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 5)
?h2o.gbm
HyperParam = list(ntrees = c(100,200,400,800,1600,3200,6400,12800))
gridTreesSearch <- h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_TOT_TREES",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 5,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
learn_rate_annealing = 0.95,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 0.0001,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 5)
gridTreesSearch
h2o.shutdown()
h2o.clusterIsUp()
# Cleanup the environment and load libraries
rm(list = ls(all.names = TRUE))
library(h2o)
# Initialize H2O
h2o.init(nthreads = -1, min_mem_size = "6G")
# Read Train & Test into H2O
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Transform "loss" variable to Log(loss)
AllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)
# No loss variable in AllStateTest.hex
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
# Split Training Data into Train & Validation set
SplitFrames = h2o.splitFrame(data = AllStateTrain.hex, ratios = 0.7, seed = 1)
ModTrain.hex = h2o.assign(data = SplitFrames[[1]], key = "ModTrain.hex")
ModTest.hex = h2o.assign(data = SplitFrames[[2]], key = "ModTest.hex")
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(ModTrain.hex), DepAttrib)
HyperParam = list(ntrees = c(100,200,400,800,1600,3200,6400,12800))
gridTreesSearch <- h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_TOT_TREES",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 10,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
learn_rate_annealing = 0.95,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 0.0001,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 5)
gridTreesSearch
gridTrees = h2o.getGrid(grid_id = "GRID_TOT_TREES", sort_by = "mse")
# Check error metrics for the models
lapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
lapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
# Check the spread between Valid and Train metrics: MAE and RMSE
lapply(lapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y["valid"] - y["train"]})
lapply(lapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y["valid"] - y["train"]})
gridTreesSearch
HyperParam = list(ntrees = seq(40,240,20))
HyperParam
HyperParam = list(ntrees = seq(40,300,20))
HyperParam
length(HyperParam)
length(HyperParam[[1]])
HyperParam = list(ntrees = seq(40,300,20))
gridTreesSearch2 <- h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_TOT_TREES_2",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 10,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
learn_rate_annealing = 0.95,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 0.0001,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 5)
gridTreesSearch2
gridTreesSearch
gridTreesSearch2
lapply(gridTrees2@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
lapply(gridTreesSearch2@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
lapply(gridTreesSearch2@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
gridTreesSearch2
gridTrees = h2o.getGrid(grid_id = "GRID_TOT_TREES_2", sort_by = "mse")
# Get grid:
gridTrees = h2o.getGrid(grid_id = "GRID_TOT_TREES_2", sort_by = "mse")
# Check error metrics for the models
lapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
lapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})
# Check the spread between Valid and Train metrics: MAE and RMSE
lapply(lapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y["valid"] - y["train"]})
lapply(lapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y["valid"] - y["train"]})
TopModel = h2o.getModel(model_id = gridTrees@model_ids[[1]])
ImpVar = h2o.varimp(TopModel)
ImpVar = h2o.varimp(TopModel)
write.csv(x = as.data.frame(x = ImpVar), file = "FEATURES_GBM_GRID_NTree.csv", row.names = FALSE)
ImpVar
# Generate Predictions
predGBM = h2o.predict(object = TopModel, newdata = AllStateTest.hex)
# Following step is required only for Log(loss) predictions
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_28102016_01.csv", row.names = FALSE)
# Cleanup the environment and load libraries
rm(list = ls(all.names = TRUE))
library(h2o)
# Initialize H2O
h2o.init(nthreads = -1, min_mem_size = "6G")
# Read Train & Test into H2O
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Transform "loss" variable to Log(loss)
AllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)
# No loss variable in AllStateTest.hex
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
# Split Training Data into Train & Validation set
SplitFrames = h2o.splitFrame(data = AllStateTrain.hex, ratios = 0.7, seed = 1)
ModTrain.hex = h2o.assign(data = SplitFrames[[1]], key = "ModTrain.hex")
ModTest.hex = h2o.assign(data = SplitFrames[[2]], key = "ModTest.hex")
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(ModTrain.hex), DepAttrib)
HyperParam = list(learn_rate = c(0.01,0.2,0.01))
length(HyperParam[[1]])
HyperParam
HyperParam = list(learn_rate = seq(0.01,0.2,0.01))
HyperParam
gridTreesSearch <- h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_LEARNRATE_NOANNEAL",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 10,
ntrees = 200,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
#learn_rate_annealing = 0.95,
## Early stopping configuration
gridTreesSearch <- h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_LEARNRATE_NOANNEAL",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 10,
ntrees = 200,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
#learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
## Early stopping configuration
#learn_rate_annealing = 0.95,
stopping_rounds = 3,
stopping_tolerance = 0.0001,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 5)
gridLRate <-       h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_LEARNRATE_NOANNEAL",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 10,
ntrees = 200,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
#learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
#learn_rate_annealing = 0.95,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 0.0001,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 5)
gridLRate
HyperParam = list(learn_rate = seq(0.01,0.2,0.01), ntrees = c(150,200,300,400,500,750,1000,5000))
gridLRate2 <-       h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "GRID_LEARNRATE_NOANNEAL_2",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
max_depth = 10,
#ntrees = 200,
## smaller learning rate is better
## Due to learning_rate_annealing, we can start with a bigger learning rate
#learn_rate = 0.05,
## learning rate annealing: learning_rate shrinks by 1% after every tree
#learn_rate_annealing = 0.95,
## Early stopping configuration
stopping_rounds = 3,
stopping_tolerance = 0.0001,
stopping_metric = "MSE",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 10 trees to make early stopping reproducible
score_tree_interval = 5)
gridLRate2
h2o.clusterIsUp()
h2o.clusterInfo()
getwd()
#####################################################################################
#####################################################################################
##### KAGGLE COMPETITION DATASET: ALL STATE INSURANCE LOSS PREDICTION
#####################################################################################
#####################################################################################
##### FEATURE ENGINEERING: PCA FOR NUMERIC VARIABLES | loss >> LOG(loss) TRANSFORM
##### FRAMEWORK: H2O
##### ALGORITHM: GBM - GRADIANT BOOSTING MACHINE
#####################################################################################
# Cleanup the environment and load libraries
rm(list = ls(all.names = TRUE))
library(h2o)
# Initialize H2O :: DELL_LAPTOP
h2o.init(nthreads = -1, min_mem_size = "6G")
# Initialize H2O :: LENOVO_AIO
#h2o.init(nthreads = -1, min_mem_size = "3500M")
# Read Train & Test into H2O - DELL_LAPTOP
AllStateTrain.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
AllStateTest.hex = h2o.uploadFile(path = "C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Read Train & Test into H2O - LENOVO_AIO
#AllStateTrain.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/train.csv", destination_frame = "AllStateTrain.hex", header = TRUE)
#AllStateTest.hex = h2o.uploadFile(path = "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/test.csv", destination_frame = "AllStateTest.hex", header = TRUE)
# Transform "loss" variable to Log(loss)
AllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)
# No loss variable in AllStateTest.hex
# Save and remove id column from Train & Test dataset
TrainId = AllStateTrain.hex$id
TestId = AllStateTest.hex$id
AllStateTrain.hex$id = NULL
AllStateTest.hex$id = NULL
# Variable names
vFactors = paste0("cat", 1:116)
vNumbers = paste0("cont", 1:14)
# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components
PCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = "PCA_01", transform = "STANDARDIZE", seed = 1)
# Remove Continuous variable and add PCA Score - Train data frame
AllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = "AllStateTrainPCA.hex")
AllStateTrain.hex[ ,vNumbers] = NULL
AllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])
# Remove Continuous variable and add PCA Score - Test Data Frame
AllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = "AllStateTestPCA.hex")
AllStateTest.hex[ ,vNumbers] = NULL
AllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])
# Split Training Data into Train & Validation set
SplitFrames = h2o.splitFrame(data = AllStateTrain.hex, ratios = 0.7, seed = 1)
ModTrain.hex = h2o.assign(data = SplitFrames[[1]], key = "ModTrain.hex")
ModTest.hex = h2o.assign(data = SplitFrames[[2]], key = "ModTest.hex")
# Attribute split for further modelling.
DepAttrib = "loss"
IndAttrib = setdiff(names(ModTrain.hex), DepAttrib)
HyperParam = list(learn_rate = 0.0125, ntrees = 1000, max_depth = 10)
KaggelModel1 <-       h2o.grid(algorithm = "gbm",
x = IndAttrib,
y = DepAttrib,
training_frame = ModTrain.hex,
grid_id = "KAGGLE_MODEL_1",
hyper_params = HyperParam,
validation_frame = ModTest.hex,
seed = 1,
distribution = "gaussian",
stopping_rounds = 5,
stopping_tolerance = 0.0001,
stopping_metric = "deviance",
## sample 80% of rows per tree
sample_rate = 0.8,
## sample 80% of columns per split
col_sample_rate = 0.8,
## score every 5 trees to make early stopping reproducible
score_tree_interval = 5)
TopModel = h2o.getModel(model_id = KaggelModel1@model_ids[[1]])
# Generate Predictions
predGBM = h2o.predict(object = TopModel, newdata = AllStateTest.hex)
# Following step is required only for Log(loss) predictions
predGBM = h2o.exp(predGBM)
dfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))
names(dfGBMPredictions) = c("id", "loss")
write.csv(x = dfGBMPredictions, file = "H2O_GBM_29102016_02.csv", row.names = FALSE)
# Initialize H2O :: DELL_LAPTOP
h2o.init(nthreads = -1, min_mem_size = "6G")
h2o.clusterIsUp()
h2o.clusterInfo()
h2o.init()
h2o.saveModel(object = "KAGGLE_MODEL_1_model_0")
h2o.saveModel(object = h2o.getModel("KAGGLE_MODEL_1_model_0"))
q()
