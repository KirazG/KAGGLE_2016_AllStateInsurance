{
    "collab_server" : "",
    "contents" : "#####################################################################################\n#####################################################################################\n##### KAGGLE COMPETITION DATASET: ALL STATE INSURANCE LOSS PREDICTION\n#####################################################################################\n#####################################################################################\n##### FEATURE ENGINEERING: PCA FOR NUMERIC VARIABLES | loss >> LOG(loss) TRANSFORM\n##### FRAMEWORK: H2O\n##### ALGORITHM: XGB IN R - EXTREME GRADIANT BOOSTING IN R\n#####################################################################################\n\n# Cleanup the environment and load libraries\nrm(list = ls(all.names = TRUE))\nlibrary(h2o)\nlibrary(dplyr)\nlibrary(xgboost)\nlibrary(dummies)\n\n# Initialize H2O :: DELL_LAPTOP\n#h2o.init(nthreads = -1, min_mem_size = \"5G\")\n\n# Initialize H2O :: LENOVO_AIO\nh2o.init(nthreads = -1, min_mem_size = \"3500M\")\n\n# Read Train & Test into H2O - DELL_LAPTOP\n#AllStateTrain.hex = h2o.uploadFile(path = \"C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv\", destination_frame = \"AllStateTrain.hex\", header = TRUE)\n#AllStateTest.hex = h2o.uploadFile(path = \"C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv\", destination_frame = \"AllStateTest.hex\", header = TRUE)\n\n# Read Train & Test into H2O - LENOVO_AIO\nAllStateTrain.hex = h2o.uploadFile(path = \"D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/train.csv\", destination_frame = \"AllStateTrain.hex\", header = TRUE)\nAllStateTest.hex = h2o.uploadFile(path = \"D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/test.csv\", destination_frame = \"AllStateTest.hex\", header = TRUE)\n\n# Save and remove id column from Train & Test dataset\nTrainId = AllStateTrain.hex$id\nTestId = AllStateTest.hex$id\nAllStateTrain.hex$id = NULL\nAllStateTest.hex$id = NULL\n\n# Variable names\nvFactors = paste0(\"cat\", 1:116)\nvNumbers = paste0(\"cont\", 1:14)\n\n# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components\nPCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = \"PCA_01\", transform = \"STANDARDIZE\", seed = 1)\n\n# Remove Continuous variable and add PCA Score - Train data frame\nAllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = \"AllStateTrainPCA.hex\")\nAllStateTrain.hex[ ,vNumbers] = NULL\nAllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])\n\n# Remove Continuous variable and add PCA Score - Test Data Frame\nAllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = \"AllStateTestPCA.hex\")\nAllStateTest.hex[ ,vNumbers] = NULL\nAllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])\n\ndfAllStateTrain = as.data.frame(x = AllStateTrain.hex)\ndfAllStateTest = as.data.frame(x = AllStateTest.hex)\n# Shutdown H2O after PCA is done !\nh2o.shutdown(prompt = FALSE)\n\n# Transform \"loss\" variable to Log(loss)\ndfAllStateTrain$loss = log(dfAllStateTrain$loss)\n# No loss variable in AllStateTest.hex\n\n# Convert Train and Test data frames into numerical\ndfAllStateTrain = dummy.data.frame(data = dfAllStateTrain, names = names(dfAllStateTrain)[1:116])\ndfAllStateTest = dummy.data.frame(data = dfAllStateTest, names = names(dfAllStateTest)[1:116])\n\n\n# Split Training Data into Train & Validation set\nset.seed(1)\nSRS = sample(x = 1:nrow(dfAllStateTrain), size = 0.7*nrow(dfAllStateTrain))\ndfModTrain = dfAllStateTrain[SRS, ]\ndfModTest = dfAllStateTrain[-SRS, ]\n\n# Attribute split for further modelling.\nDepAttrib = \"loss\"\nIndAttrib = setdiff(names(dfModTrain), DepAttrib)\n\n#####################################################################################\n##### IMPLEMENTING XGB ON ALL FEATURES                                          #####\n#####################################################################################\n\n\nxgbParams = list(objective = \"reg:linear\", eta = 0.3, max.depth = 5, \n                 subsample = 0.5, colsample_bytree = 0.5)\n\nxgbModel001 = xgboost(data = data.matrix(frame = dfModTrain), label = \"loss\", params = xbgParams, \n                      nrounds = 1, verbose = 2, early.stop.round = 3,maximize = TRUE )\n",
    "created" : 1478431824960.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "767312503",
    "id" : "DDC3F2DB",
    "lastKnownWriteTime" : 1478432165,
    "last_content_update" : 1478432165327,
    "path" : "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/Modelling/F1_AllState_Models_XGB_TUNING.R",
    "project_path" : "F1_AllState_Models_XGB_TUNING.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}