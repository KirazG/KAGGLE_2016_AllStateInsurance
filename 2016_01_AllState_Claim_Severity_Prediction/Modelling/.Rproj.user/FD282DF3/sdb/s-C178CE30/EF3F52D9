{
    "collab_server" : "",
    "contents" : "#####################################################################################\n#####################################################################################\n##### KAGGLE COMPETITION DATASET: ALL STATE INSURANCE LOSS PREDICTION\n#####################################################################################\n#####################################################################################\n##### FEATURE ENGINEERING: PCA FOR NUMERIC VARIABLES | loss >> LOG(loss) TRANSFORM\n##### FRAMEWORK: H2O\n##### ALGORITHM: GBM - GRADIANT BOOSTING MACHINE\n#####################################################################################\n\n# Cleanup the environment and load libraries\nrm(list = ls(all.names = TRUE))\nlibrary(h2o)\n\n# Initialize H2O :: DELL_LAPTOP\n#h2o.init(nthreads = -1, min_mem_size = \"6G\")\n\n# Initialize H2O :: LENOVO_AIO\nh2o.init(nthreads = -1, min_mem_size = \"3500M\")\n\n# Read Train & Test into H2O - DELL_LAPTOP\n#AllStateTrain.hex = h2o.uploadFile(path = \"C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/train.csv\", destination_frame = \"AllStateTrain.hex\", header = TRUE)\n#AllStateTest.hex = h2o.uploadFile(path = \"C:/02 KAGGLE/2016_AllState_Claim_Severity_Prediction/test.csv\", destination_frame = \"AllStateTest.hex\", header = TRUE)\n\n# Read Train & Test into H2O - LENOVO_AIO\nAllStateTrain.hex = h2o.uploadFile(path = \"D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/train.csv\", destination_frame = \"AllStateTrain.hex\", header = TRUE)\nAllStateTest.hex = h2o.uploadFile(path = \"D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/test.csv\", destination_frame = \"AllStateTest.hex\", header = TRUE)\n\n\n# Transform \"loss\" variable to Log(loss)\nAllStateTrain.hex$loss = h2o.log(AllStateTrain.hex$loss)\n# No loss variable in AllStateTest.hex\n\n# Save and remove id column from Train & Test dataset\nTrainId = AllStateTrain.hex$id\nTestId = AllStateTest.hex$id\nAllStateTrain.hex$id = NULL\nAllStateTest.hex$id = NULL\n\n# Variable names\nvFactors = paste0(\"cat\", 1:116)\nvNumbers = paste0(\"cont\", 1:14)\n\n# Perform PCA on numeric data >> Remove original attributes >> Select & attach important components\nPCA = h2o.prcomp(training_frame = AllStateTrain.hex, x = vNumbers, k = length(vNumbers), model_id = \"PCA_01\", transform = \"STANDARDIZE\", seed = 1)\n\n# Remove Continuous variable and add PCA Score - Train data frame\nAllStateTrainPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTrain.hex), key = \"AllStateTrainPCA.hex\")\nAllStateTrain.hex[ ,vNumbers] = NULL\nAllStateTrain.hex = h2o.cbind(AllStateTrain.hex, AllStateTrainPCA.hex[ ,1:12])\n\n# Remove Continuous variable and add PCA Score - Test Data Frame\nAllStateTestPCA.hex = h2o.assign(data = h2o.predict(object = PCA, newdata = AllStateTest.hex), key = \"AllStateTestPCA.hex\")\nAllStateTest.hex[ ,vNumbers] = NULL\nAllStateTest.hex = h2o.cbind(AllStateTest.hex, AllStateTestPCA.hex[ ,1:12])\n\n# Split Training Data into Train & Validation set\nSplitFrames = h2o.splitFrame(data = AllStateTrain.hex, ratios = 0.7, seed = 1)\nModTrain.hex = h2o.assign(data = SplitFrames[[1]], key = \"ModTrain.hex\")\nModTest.hex = h2o.assign(data = SplitFrames[[2]], key = \"ModTest.hex\")\n\n# Attribute split for further modelling.\nDepAttrib = \"loss\"\nIndAttrib = setdiff(names(ModTrain.hex), DepAttrib)\n\n#####################################################################################\n##### GBM TUNING IMPLEMENTATION                                                 #####\n#####################################################################################\n\n-------------------------------------------------------------------------------------\n##### GBM: TREE DEPTH TUNING                                                    #####\n----------------------------------------------------------h2o.gri---------------------------\n\nHyperParam = list(max_depth = seq(1,29,2))\n\ngridDepthSearch <- h2o.grid(algorithm = \"gbm\", \n                            x = IndAttrib,\n                            y = DepAttrib, \n                            training_frame = ModTrain.hex, \n                            grid_id = \"GRID_DEPTH\",\n                            hyper_params = HyperParam,\n                            validation_frame = ModTest.hex, \n                            seed = 1, \n                            distribution = \"gaussian\",\n                            ntrees = 7500,\n                            ## smaller learning rate is better\n                            ## Due to learning_rate_annealing, we can start with a bigger learning rate\n                            learn_rate = 0.05,                                                         \n                            ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                            learn_rate_annealing = 0.99,\n                            ## Early stopping configuration\n                            stopping_rounds = 3, \n                            stopping_tolerance = 1e-4,\n                            stopping_metric = \"MSE\",\n                            ## sample 80% of rows per tree\n                            sample_rate = 0.8,\n                            ## sample 80% of columns per split\n                            col_sample_rate = 0.8,\n                            ## score every 10 trees to make early stopping reproducible\n                            score_tree_interval = 10)\n\n# Get grid:\ngridDepth = h2o.getGrid(grid_id = \"GRID_DEPTH\", sort_by = \"mse\")\n\n# Check error metrics for the models\nlapply(gridDepth@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})\nlapply(gridDepth@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})\n\n# Check the spread between Valid and Train metrics: MAE and RMSE\nlapply(lapply(gridDepth@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y[\"valid\"] - y[\"train\"]})\n\nlapply(lapply(gridDepth@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y[\"valid\"] - y[\"train\"]})\n\n### MAX_DEPTH TUNING OUTCOME:\n###     Based on Top 5 models - max_depth range = 9-17\n###     Based on Top 3 models - max_depth range = 11-15\n###     Based on Observation  - max_depth range = 9-15\n\nTopModel = h2o.getModel(model_id = gridDepth@model_ids[[1]])\nTopModel = h2o.getModel(model_id = \"GRID_DEPTH_model_4\")\n\n# Save the features from all 10 models\nfor(i in 1:10)\n{\n    ImpVar = h2o.varimp(h2o.getModel(model_id = gridDepth@model_ids[[i]]))\n    write.csv(x = as.data.frame(x = ImpVar), file = paste0(\"FEATURES_GBM_GRID_MaxDepth_\", i, \".csv\"), row.names = FALSE)\n}\n\n# Generate Predictions\npredGBM = h2o.predict(object = TopModel, newdata = AllStateTest.hex)\n# Following step is required only for Log(loss) predictions\npredGBM = h2o.exp(predGBM)\ndfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))\nnames(dfGBMPredictions) = c(\"id\", \"loss\")\nwrite.csv(x = dfGBMPredictions, file = \"H2O_GBM_26102016_01.csv\", row.names = FALSE)\n# THE SUBMISSION SCORED ~ 1173.XXXXX - NOT THE BEST. MORE TUNING REQUIRED.\n\n\n-------------------------------------------------------------------------------------\n##### GBM: # OF TREES TUNING                                                    #####\n-------------------------------------------------------------------------------------\n\nHyperParam = list(ntrees = c(100,200,400,800,1600,3200,6400,12800))\n\ngridTreesSearch <- h2o.grid(algorithm = \"gbm\", \n                            x = IndAttrib,\n                            y = DepAttrib, \n                            training_frame = ModTrain.hex, \n                            grid_id = \"GRID_TOT_TREES\",\n                            hyper_params = HyperParam,\n                            validation_frame = ModTest.hex, \n                            seed = 1, \n                            distribution = \"gaussian\",\n                            max_depth = 10,\n                            ## smaller learning rate is better\n                            ## Due to learning_rate_annealing, we can start with a bigger learning rate\n                            learn_rate = 0.05,                                                         \n                            ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                            learn_rate_annealing = 0.95,\n                            ## Early stopping configuration\n                            stopping_rounds = 3, \n                            stopping_tolerance = 0.0001,\n                            stopping_metric = \"MSE\",\n                            ## sample 80% of rows per tree\n                            sample_rate = 0.8,\n                            ## sample 80% of columns per split\n                            col_sample_rate = 0.8,\n                            ## score every 10 trees to make early stopping reproducible\n                            score_tree_interval = 5)\n\n# Get grid:\ngridTrees = h2o.getGrid(grid_id = \"GRID_TOT_TREES\", sort_by = \"mse\")\n\n# Check error metrics for the models\nlapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})\nlapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})\n\n# Check the spread between Valid and Train metrics: MAE and RMSE\nlapply(lapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y[\"valid\"] - y[\"train\"]})\n\nlapply(lapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y[\"valid\"] - y[\"train\"]})\n\n### OBSERVATION: BASED ON gridTrees - BUILDING >200 TREES IS NOT RESULTING IN ANY IMPORVED ACCURACY/MAE/MSE REDUCTION. SUBSEQUENT TUNING ATTEMPT TO FIND # OF TREES IN MORE GRANULAR FASHION.\n\nHyperParam = list(ntrees = seq(40,300,20))\n\ngridTreesSearch2 <- h2o.grid(algorithm = \"gbm\", \n                            x = IndAttrib,\n                            y = DepAttrib, \n                            training_frame = ModTrain.hex, \n                            grid_id = \"GRID_TOT_TREES_2\",\n                            hyper_params = HyperParam,\n                            validation_frame = ModTest.hex, \n                            seed = 1,\n                            distribution = \"gaussian\",\n                            max_depth = 10,\n                            ## smaller learning rate is better\n                            ## Due to learning_rate_annealing, we can start with a bigger learning rate\n                            learn_rate = 0.05,                                                         \n                            ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                            learn_rate_annealing = 0.95,\n                            ## Early stopping configuration\n                            stopping_rounds = 3, \n                            stopping_tolerance = 0.0001,\n                            stopping_metric = \"MSE\",\n                            ## sample 80% of rows per tree\n                            sample_rate = 0.8,\n                            ## sample 80% of columns per split\n                            col_sample_rate = 0.8,\n                            ## score every 10 trees to make early stopping reproducible\n                            score_tree_interval = 5)\n\n# Get grid:\ngridTrees = h2o.getGrid(grid_id = \"GRID_TOT_TREES_2\", sort_by = \"mse\")\n\n# Check error metrics for the models\nlapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)})\nlapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)})\n\n# Check the spread between Valid and Train metrics: MAE and RMSE\nlapply(lapply(gridTrees@model_ids, function(x) {h2o.mae(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y[\"valid\"] - y[\"train\"]})\n\nlapply(lapply(gridTrees@model_ids, function(x) {h2o.rmse(object = h2o.getModel(x), train = TRUE, valid = TRUE)}), function(y){y[\"valid\"] - y[\"train\"]})\n\n### \"NTREE\" TUNING OUTCOME:\n###     MAE/RMSE are minimized for 180 trees @ depth = 10 and learn_rate = 0.05 with 5% annealing\n###     NTREE  RANGE (STRICT): 50-180\n###     NTREE  RANGE (RELAXED): 50-250\n\nTopModel = h2o.getModel(model_id = gridTrees@model_ids[[1]])\n\n# Save the features from all 10 models\n\nImpVar = h2o.varimp(TopModel)\nwrite.csv(x = as.data.frame(x = ImpVar), file = \"FEATURES_GBM_GRID_NTree.csv\", row.names = FALSE)\n\n# Generate Predictions\npredGBM = h2o.predict(object = TopModel, newdata = AllStateTest.hex)\n# Following step is required only for Log(loss) predictions\npredGBM = h2o.exp(predGBM)\ndfGBMPredictions = as.data.frame(h2o.cbind(TestId, predGBM))\nnames(dfGBMPredictions) = c(\"id\", \"loss\")\nwrite.csv(x = dfGBMPredictions, file = \"H2O_GBM_28102016_01.csv\", row.names = FALSE)\n\n### TOP MODEL SCORE: 1387.43941\n### HIGHER NTREE MIGHT BE REQUIRED WITH LOWER LEARNING RATE. ALSO DEPTH SEEM TO HAVE A HIGHER IMPACT.\n\n\n-------------------------------------------------------------------------------------\n##### GBM: LEARNING RATE TUNING (NO ANNEALING)                                  #####\n-------------------------------------------------------------------------------------\n    \nHyperParam = list(learn_rate = seq(0.01,0.2,0.01))\n\ngridLRate <-       h2o.grid(algorithm = \"gbm\", \n                            x = IndAttrib,\n                            y = DepAttrib, \n                            training_frame = ModTrain.hex, \n                            grid_id = \"GRID_LEARNRATE_NOANNEAL\",\n                            hyper_params = HyperParam,\n                            validation_frame = ModTest.hex, \n                            seed = 1, \n                            distribution = \"gaussian\",\n                            max_depth = 10,\n                            ntrees = 200,\n                            ## smaller learning rate is better\n                            ## Due to learning_rate_annealing, we can start with a bigger learning rate\n                            #learn_rate = 0.05,                                                         \n                            ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                            #learn_rate_annealing = 0.95,\n                            ## Early stopping configuration\n                            stopping_rounds = 3, \n                            stopping_tolerance = 0.0001,\n                            stopping_metric = \"MSE\",\n                            ## sample 80% of rows per tree\n                            sample_rate = 0.8,\n                            ## sample 80% of columns per split\n                            col_sample_rate = 0.8,\n                            ## score every 10 trees to make early stopping reproducible\n                            score_tree_interval = 5)\n\n# tuning iteration 2: RandomDiscrete Tuning\n\nHyperParam = list(learn_rate = seq(0.01,0.15,0.01), \n                  ntrees = c(900,1200))\n\ngridLRate3 <-       h2o.grid(algorithm = \"gbm\", \n                            x = IndAttrib,\n                            y = DepAttrib, \n                            training_frame = ModTrain.hex, \n                            grid_id = \"GRID_LEARNRATE_NOANNEAL_3\",\n                            hyper_params = HyperParam,\n                            validation_frame = ModTest.hex, \n                            seed = 1, \n                            distribution = \"gaussian\",\n                            max_depth = 10,\n                            #ntrees = 200,\n                            ## smaller learning rate is better\n                            ## Due to learning_rate_annealing, we can start with a bigger learning rate\n                            #learn_rate = 0.05,                                                         \n                            ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                            #learn_rate_annealing = 0.95,\n                            ## Early stopping configuration\n                            ##stopping_rounds = 3, \n                            ##stopping_tolerance = 0.0001,\n                            ##stopping_metric = \"MSE\",\n                            ## sample 80% of rows per tree\n                            sample_rate = 0.8,\n                            ## sample 80% of columns per split\n                            col_sample_rate = 0.8,\n                            ## score every 5 trees to make early stopping reproducible\n                            score_tree_interval = 5,\n                            ## Search Strategy for Hyperparameter space\n                            search_criteria =  list(strategy = \"RandomDiscrete\", \n                                                    max_runtime_secs = 12600,\n                                                    stopping_rounds = 5, \n                                                    stopping_tolerance = 0.0001,\n                                                    stopping_metric = \"deviance\"))\n\n\nG1 <- h2o.getGrid(\"GRID_LEARNRATE_NOANNEAL_3\",sort_by = \"mse\")\n\nM1 <- h2o.getModel(model_id = G1@model_ids[[1]])\n\nlapply(lapply(G1@model_ids, function(x){h2o.getModel(model_id = x)}), \n       function(M1){M1@model$scoring_history$number_of_trees[which.min(M1@model$scoring_history$validation_mae)]})\n\n#-------------------------------------------------------\n### Stratified Tunning For NTrees: Cartesian Grid Search\n#-------------------------------------------------------\n\n### Strata 1: 0.00-0.05\nHyperParam = list(learn_rate = seq(0.005,0.05,0.005), \n                  ntrees = c(600))\n\ngridLRate4 <-       h2o.grid(algorithm = \"gbm\", \n                             x = IndAttrib,\n                             y = DepAttrib, \n                             training_frame = ModTrain.hex, \n                             grid_id = \"GRID_LEARNRATE_NOANNEAL_4\",\n                             hyper_params = HyperParam,\n                             validation_frame = ModTest.hex, \n                             seed = 1, \n                             distribution = \"gaussian\",\n                             max_depth = 10,\n                             #ntrees = 200,\n                             ## smaller learning rate is better\n                             ## Due to learning_rate_annealing, we can start with a bigger learning rate\n                             #learn_rate = 0.05,                                                         \n                             ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                             #learn_rate_annealing = 0.95,\n                             ## Early stopping configuration\n                             stopping_rounds = 5, \n                             stopping_tolerance = 0.0001,\n                             stopping_metric = \"deviance\",\n                             ## sample 80% of rows per tree\n                             sample_rate = 0.8,\n                             ## sample 80% of columns per split\n                             col_sample_rate = 0.8,\n                             ## score every 5 trees to make early stopping reproducible\n                             score_tree_interval = 5)\n\n### Strata 1: 0.055-0.100\nHyperParam = list(learn_rate = seq(0.055,0.100,0.005), \n                  ntrees = c(300))\n\ngridLRate5 <-       h2o.grid(algorithm = \"gbm\", \n                             x = IndAttrib,\n                             y = DepAttrib, \n                             training_frame = ModTrain.hex, \n                             grid_id = \"GRID_LEARNRATE_NOANNEAL_5\",\n                             hyper_params = HyperParam,\n                             validation_frame = ModTest.hex, \n                             seed = 1, \n                             distribution = \"gaussian\",\n                             max_depth = 10,\n                             #ntrees = 200,\n                             ## smaller learning rate is better\n                             ## Due to learning_rate_annealing, we can start with a bigger learning rate\n                             #learn_rate = 0.05,                                                         \n                             ## learning rate annealing: learning_rate shrinks by 1% after every tree \n                             #learn_rate_annealing = 0.95,\n                             ## Early stopping configuration\n                             stopping_rounds = 5, \n                             stopping_tolerance = 0.0001,\n                             stopping_metric = \"deviance\",\n                             ## sample 80% of rows per tree\n                             sample_rate = 0.8,\n                             ## sample 80% of columns per split\n                             col_sample_rate = 0.8,\n                             ## score every 5 trees to make early stopping reproducible\n                             score_tree_interval = 5)\n\n### Extracting the information from 4 GRID SEARCHES\n\nGRID_ID = c(\"GRID_LEARNRATE_NOANNEAL_5\", \"GRID_LEARNRATE_NOANNEAL_4\", \"GRID_LEARNRATE_NOANNEAL_3\", \"GRID_LEARNRATE_NOANNEAL_2\")\ni=1;\nGRID = h2o.getGrid(GRID_ID[i])\nGRID_MODELS = h2o.getModel(model_id = GRID@model_ids)\n\n",
    "created" : 1477640888772.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "959781111",
    "id" : "EF3F52D9",
    "lastKnownWriteTime" : 1477716779,
    "last_content_update" : 1477716814870,
    "path" : "D:/10 CONTINUOUS LEARNING/83 KAGGLE/KAGGLE_COMPETITIONS/2016_01_AllState_Claim_Severity_Prediction/Modelling/AllState_Models_GBM_TUNING.R",
    "project_path" : "AllState_Models_GBM_TUNING.R",
    "properties" : {
        "notebook_format" : "pdf_document",
        "source_window_id" : ""
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}